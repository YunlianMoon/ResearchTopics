# Language

#### Reference

[Machine Learning and having it deep and structured (2015,Fall)](http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLSD15_2.html)

#### Table of Contents
 - Word
   - 1 of N encoding
   - continuous bed of word (CBOW)
   - skip-gram
   - dimension of 'other'
   - word hashing
 - Word Squence
   - deep structured semantic model (DSSM)
     - bag of word
   - recursive deep model
   - paragraph vector
 
#### word

<div align=center>
  <img src="https://github.com/YunlianMoon/ResearchTopics/blob/master/Language/images/word_1.png" width="22%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/arrow.jpg" width="2%" />
  <img src="https://github.com/YunlianMoon/ResearchTopics/blob/master/Language/images/word_2.png" width="22%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/arrow.jpg" width="2%" />
  <img src="https://github.com/YunlianMoon/ResearchTopics/blob/master/Language/images/word_3.png" width="22%" />
  <img src="https://github.com/YunlianMoon/AILibrary/blob/master/DeepLearning/Attention/images/arrow.jpg" width="2%" />
  <img src="https://github.com/YunlianMoon/ResearchTopics/blob/master/Language/images/word_4.png" width="22%" /><br />
  Word Vector/Word Embedding
</div>

#### word sequence




